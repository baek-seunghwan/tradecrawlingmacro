
## 1) SQL을 활용한 경험이 있으신가요? (개선본)

저는 MySQL을 활용해 외부에서 수집한 시계열 데이터를 저장·관리한 경험이 있습니다.
Selenium을 사용해 TradingView에서 종목별·주기별(연·월·주·일·시간·10분) 데이터를 CSV로 자동 다운로드하고, Python에서 시간 포맷을 표준화한 뒤 숫자형 데이터의 콤마 제거, `nan` 값 처리, 결측 행 제거 등의 정제 과정을 거쳐 MySQL에 적재했습니다.

데이터 처리 과정에서 **시간 포맷이 서로 다른 CSV가 섞여 들어오면서 중복 적재나 조회 오류가 발생**한 적이 있습니다. 이를 해결하기 위해 Python 단계에서 모든 시간을 `DATETIME` 기준으로 통일하고, 테이블 설계 시 `symbol + timeframe + time`을 복합 기본 키로 설정해 데이터의 유일성을 보장했습니다. 또한 `INSERT … ON DUPLICATE KEY UPDATE`를 사용해 재수집·재실행 시에도 중복 없이 최신 값으로 갱신되도록 구현했습니다.

적재 과정에서는 성능을 고려해 배치 단위로 insert를 수행했고, 트랜잭션을 활용해 오류 발생 시 rollback 되도록 구성했습니다. 아울러 DB가 외부 네트워크에 위치해 있어 SSH 터널을 구성해 안전하게 접속한 뒤 데이터 적재를 수행했습니다.
이 경험을 통해 SQL을 단순 조회가 아닌, **데이터 품질·정합성·재실행 안정성을 보장하는 파이프라인의 핵심 구성 요소로 활용**하는 방법을 이해하게 되었습니다.

---

## 2) 데이터 모델링에 대해 설명해 주실 수 있나요? (비즈니스 기여 추가 개선본)

데이터 모델링은 데이터의 구조를 정의해 **정합성(정확함)과 활용성(쉽게 쓰임)을 높이고**, 결국 분석/서비스 의사결정을 빠르게 만드는 과정이라고 이해하고 있습니다.

제가 진행한 프로젝트에서는 시계열 데이터의 특성을 고려해 `symbol, timeframe, time`을 기준으로 복합 기본 키를 설계했습니다. 이를 통해 동일 종목·주기의 중복 적재를 방지하고, 시간 기준 조회가 용이하도록 했습니다. 또한 가격(Open, High, Low, Close)과 거래량, 보조지표(RSI, MACD)를 컬럼 단위로 분리해 관리해 **분석 시 필요한 지표를 빠르게 조합하고, 쿼리 조건을 명확히 적용**할 수 있게 했습니다.

결과적으로, 데이터 모델링을 통해 데이터가 누적될수록 **조회·집계가 안정적이고 일관되게 동작**하도록 만들었고, 이는 데이터 마트/리포팅 관점에서 반복 작업을 줄이는 데 기여한다고 생각합니다.

---

## 3) 데이터 처리 과정에서 어떤 도구를 사용하셨나요? (구체 성과/최적화 추가 개선본)

주요 도구로는 Python, Selenium, MySQL을 사용했습니다.
수집 단계에서는 Selenium으로 웹 자동화를 구현했고, 처리 단계에서는 Python으로 CSV 파싱 및 전처리(시간 표준화, 숫자 변환, 결측 제거)를 수행했습니다. 저장 단계에서는 MySQL에서 테이블 생성, 복합 기본 키 및 인덱스 설정, 배치 insert 등을 적용했습니다. 또한 외부 DB 접속을 위해 SSH 터널링을 구성해 보안 환경에서 작업했습니다.

특히 성능 측면에서는 단건 insert 대신 `executemany()` 기반 배치 적재로 처리 시간을 줄이려고 했고, 데이터 정합성 측면에서는 복합 PK와 UPSERT로 **재실행에도 데이터가 안정적으로 유지되는 구조**를 만들었습니다. 또한 다운로드/수집 과정에서 간헐적으로 누락되는 케이스를 대비해 예외 처리와 재시도 흐름을 두어 자동화 안정성을 높였습니다.

---

## 4) 대용량 데이터 처리 경험이 있으신가요? (향후 기술 스택 언급 추가 개선본)

아직 수십억 건 규모의 데이터 처리 경험은 없지만, 여러 종목과 다양한 시간 주기를 반복 수집·적재하는 과정에서 데이터량이 빠르게 증가하는 환경을 경험했습니다. 이 과정에서 단건 insert 대신 배치 처리 방식을 적용했고, 재수집 시에도 중복 적재가 발생하지 않도록 복합 PK와 UPSERT 기반으로 설계했습니다. 이를 통해 대용량 데이터 처리 시 **성능(배치/인덱스)**과 **안정성(멱등성/트랜잭션)**을 함께 고려해야 한다는 점을 실무적으로 이해하게 되었습니다.

향후 더 큰 규모로 확장할 때는, 요구사항에 따라

* 수집/스케줄링: Airflow 같은 오케스트레이션 도구
* 처리: Spark 같은 분산 처리 프레임워크
* 저장: 파티셔닝/인덱싱 전략 또는 분석용 DW(예: BigQuery 등)
  을 검토해 병목을 분리하고 확장성을 확보하는 방향으로 접근할 계획입니다.

---

## 추가로: 면접에서 자주 나오는 꼬리질문 대비 “1문장 답”

* **왜 복합 PK를 썼나요?** → “종목·주기·시간 조합이 한 행의 유일성을 정의해서, 중복/재수집 문제를 구조적으로 차단하기 위해서입니다.”
* **왜 UPSERT를 썼나요?** → “재실행 시 멱등성을 확보하고 최신 값으로 안전하게 갱신하기 위해서입니다.”
* **대용량이면 뭐가 먼저 병목인가요?** → “수집(외부 제한)과 적재(I/O), 조회(인덱스)로 나눠 보고, 배치/인덱스/파티셔닝부터 단계적으로 접근합니다.”

---

원하시면 위 4개 답변을 **(1) 30초 버전, (2) 60초 버전, (3) 90초 버전**으로 각각 압축해서 “말로 했을 때 자연스럽게” 재정렬해 드리겠습니다.
